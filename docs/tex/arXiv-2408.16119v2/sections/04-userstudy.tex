\section{User Study Design}
\label{sec:evaluation}
To understand potential benefits and usability issues of \df, as well as users' interaction styles, we designed a user study that asks participants to reproduce exploratory data analysis sessions involving iteratively creating visualizations. 

\bpstart{Participants} After piloting and refining the study design with three volunteers, we recruited eight participants from a large company. Participants self-rated their skills (\autoref{fig:participants}) on a scale of 1 to 4 (``Novice,'' ``Intermediate,'' ``Proficient,'' and ``Expert'') in: (1) chart creation -- experience with chart authoring tools or libraries, (2) data transformation -- experience with data transformation tools and library expertise, (3) programming, and (4) AI assistants -- experience with large language models (e.g., ChatGPT~\cite{achiam2023gpt}) and prompting. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/participants.png}
    \caption{Participants' self-reported roles, expertise in chart creation, data transformation, programming, and AI assistants (1=novice, 4=expert), task completion time, and hints needed during study tasks.}
    \label{fig:participants}
\end{figure}

\bpstart{Setup and procedure} Each study session, conducted remotely with screen sharing, consisted of four sections within a 2-hour slot. After introduction, participants followed step-by-step instructions in the tutorial slides ($\sim$25 minutes). Participants then completed a practice task with the option to ask questions ($\sim$15 minutes) to test their understanding. Next, participants completed two study tasks, with only clarification questions allowed -- we recorded hints they requested. The two study tasks involved creating 16 visualizations, 12 requiring data transformation. Participants were encouraged to think aloud. We concluded with a debriefing to (1) compare participants' \df experiences with other tools, (2) understand their strategies using \df, and (3) gather impressions and suggestions for improvements. Breaks between phases were encouraged.

\bpstart{Tutorial and practice tasks} We used the global energy dataset (described in \autoref{sec:illustartive-scenarios}) for the tutorial and practice tasks. In the tutorial, participants followed detailed instructions to recreate the six visualizations from \autoref{fig:example-analysis-session} (all but chart \filled{4}).
In addition, participants also learned to inspect results and work with the AI's mistakes. 
In the practice tasks, participants were asked to do similar analyses but focusing on the electricity from nuclear power, they were further asked to create a bar chart to visualize the difference of energy produced from nuclear power between 2000 and 2020 for each country.

% The first task involved creating a line chart over time for CO2 emissions and then modifying this to show renewable energy trends, both fields of which were already in the dataset. 

% The next task was to create a trellis graph showing energy production broken out by three different sources (Electricity from fossil fuels, nuclear, and renewable sources). This involved restructuring the data so that a column showing the source of the energy was present as a field that could be used for the trellis chart. Subsequently, a chart that showed the percentage of renewable energy out of the total energy was created (which required another calculated field, based on the last chart that was produced). 

% The next task was to filter the previous chart by the top CO2 emission countries. Since this involved information not present in the chart created in the step directly previous but used a format based on the previous chart, there could be several ways to produce the resultant chart, either directly from the initial dataset or incrementally from the previous chart. 

%Finally, the last tasks involved the creation of an aggregated visualization of global renewable energy trends, which required going back to either the initial dataset or one of the subsequent iterations. It was important in the tutorial to offer multiple options by which people could perform the iterations so that people would subsequently have an opportunity to explore the iteration style for which they felt the most comfortable. 


%Then, in practice tasks, participants are asked to explore nuclear energy trends with four visualizations: (1) electricity form nuclear over time, (2) difference of nuclear between 2020 and 2000 (bar chart), (3) coloring differences based on increase/decrease, and (4) show only 5 countries with most nuclear increase. The first three tasks were presented with both task description and reference chart (i.e., they are chart reproduction task), but the last task only includes text description without reference, which we asked participants to verify and decide its correctness for probing their verification strategies.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/study-tasks.png}
    \caption{The dataset and tasks in our user study. (1) Dataset 1: Understanding top earning majors and the relation between salary and women percentage. (2) Dataset 2: Exploring movie genres with best return-on-investment values (profit vs. profit ratio) and top movies. The branching directions are added for illustration; participants developed their own iteration strategies. We refer to these target charts as C1-7 for the college dataset and M1-9 for the movies dataset.}
    \label{fig:study-tasks}
\end{figure*}
\bpstart{Study tasks} To focus on participants' iterative chart creation processes, rather than their ability to create a single chart or derive insights from exploration, we used an \emph{exploration session reproduction} approach. Participants were asked to reproduce two data exploration sessions conducted by an experienced data scientist. We wanted to see if participants could iteratively create charts with \df, without requiring them to come up with exploration objectives on the fly (otherwise we would limit our participants to highly skilled data scientists). We used two exploration sessions from David Robinson's live stream analysis of Tidy Tuesday datasets.

\autoref{fig:study-tasks}-\filled{1} shows the first data exploration session: given a dataset on college majors and income data (173 rows $\times$ 7 columns), participants were asked to create seven visualizations: two basic charts and five requiring data transformation. These visualizations progressively explored the top-earning majors and the relationship between gender ratio and major salary. 
The process required participants to derive new fields (e.g., gender ratio), filter data (e.g., top 20 earning majors), derive new data (e.g., derive top earning major categories), and perform conditional formatting (e.g., color by top 4 categories and "others"). We provided a task description and reference chart (like chart reproduction studies in \cite{ren2017chartaccent,ren2018reflecting}) for all but the last two visualizations. Without reference charts for the final two, we asked participants to verify correctness, probing their verification strategies. We did not provide iteration directions, letting participants develop iteration techniques. 


\autoref{fig:study-tasks}-\filled{2} shows the second data exploration session: given a movie dataset with budget and gross information (3281 rows $\times$ 8 columns), participants created nine visualizations. These visualizations explored movies and genres with the highest return on investment, comparing profit and profit ratios. Besides two basic box plots showing budget and worldwide gross distribution, the other seven charts required data transformation, including calculation and aggregation (average profit and profit ratio for each genre), string processing (extract year for trends), filtering (year > 2000), and partitioning and ranking (top 20 movies for each metric). We hid references for the final two charts to probe participants' verification process. In the following, we use ``chart-C$k$'' and ``chart-M$k$'' to refer to the $k$-th target charts in \autoref{fig:study-tasks} for the college and movies datasets, respectively.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/user-study-task1.png}
%     \caption{Understanding top earning majors and the relation between salary and women percentage. Two basic charts are not shown.}
%     \label{fig:study-task1}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/user-study-task2.png}
%     \caption{Exploring movies genres with best return-on-investment values (profit vs. profit ratio) and top movies. Two basic charts are omitted.}
%     \label{fig:study-task2}
% \end{figure}



% \bpstart{Task 1} For the college major dataset, we asked for charts that included Median Salary by Major Category, Median Salary by Major, Median Salary by top 20 majors, and Media Salary by top 20 Majors color-coded by Major Category. We then asked about gender diversity including Median Salary by Percentage of women, the same visualization color-coded by Major Category, and color-coded by top 4 Major Categories. 

% \bpstart{Task 2} For the movie projects dataset, we asked about Production Budgets by Genre, Worldwide Gross by Genre, Median Profit by Genre, and Profit Ratios by Genre. We then asked for Median Profit by Genre over Time, subsequently narrowed to trends after 2000, and finally, Trends after the year 2000 for movies with the highest profits. 

\definecolor{myblue}{HTML}{4DABF5}
\definecolor{myyellow}{HTML}{FFCD38}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/user-data-threads.pdf}
    \caption{Participants' workflow for study tasks in \autoref{fig:study-tasks} (C1-7 for college, M1-9 for movie). Each node represents a data table version, with \hlc[myblue!64]{blue} for initial datasets, \hlc[myyellow]{yellow} for data tables instantiating (one or multiple) target visualizations in \autoref{fig:study-tasks} (number $i$ in the node indicate the $i$-th target visualizations for the given dataset), and \hlc[gray!30]{gray} for others. Self-loop arrows indicate prompt revisions and data table updates (`$\times$2' indicates two revisions).}
    \label{fig:user-workflows}
\end{figure*}

\section{User Study Results}

Here we report user study findings including users' task completion statistics as well as their prompting, iteration and verification styles. We highlight \pquote{user quotes} and \pprompt{example prompts} in this section.

\bpstart{Task completion} All participants successfully completed all 16 visualizations (\autoref{fig:participants}): participants took less than 20 mins on average to finish the seven charts in task 1, and about 33 mins for the nine charts in task 2. Since we let participants deviate from the main exploration task (e.g., in task 2, P4 asked to sort the bar chart for top profitable movies even though it was not required), the recorded completion time is an overestimate of the actual task time. During the study, six participants asked for hints to get unstuck during tasks; we categorize them as follows:
\begin{itemize}[leftmargin=*]
\item Task clarification: P1 didn't realize that top movies were restricted to movies after 2000; P4 and P6 required hints about the difference between profit and profit ratio in task 2; P6 asked about whether the $x$-axis should be \code{Year} or \code{Date} for movie profit trends.
\item Data clarification: P6 an P8 were prompted to notice the difference between fields \code{Major} and \code{Major Category} in task 1.
\item System performance: P5 encountered a performance issue when they created large sized charts. Tn task 2, P5 created multiple bar charts with \code{Movie} mapped to the $x$-axis, resulting in bar charts containing 1300 categorical values. They were advised to reset the exploration session and resume tasks.
\item Chart encoding: P7 and P8 required hints on ``why the chart didn't render color legends'' when they didn't put a field in the color encoding; they expected to specify it only in NL input but not in the concept encoding shelf.~\footnote{In the study version, \df didn't include the feature of resolving conflicts between the users' NL and encoding shelf inputs. This feature was introduced later.}
\end{itemize}

%In general, with minimal hints, participants were able to complete the two exploration sessions themselves. 
\noindent During the debriefing, participants commented that these tasks would be much more difficult to complete with tools they are familiar with. P1, a programming expert, mentioned that they were \pquote{``obviously much faster''} with \df as it helped with data transformations. When asked about their experience comparing against chat-based AI assistants, participants noted (1) the iteration support makes it easier to create more charts and (2) the UI + NL approach in \df is more effective for communicating intent structurally. For example, P2 mentioned \pquote{``with ChatGPT, I would have to put a bit more effort to specify the instructions to get what I want, iterations here is much faster with UI.''}
%P3 \emph{``uses code interpreter separate from actual data exploration session, and transfer code to notebooks for exploration''}; 
P4 mentioned that \pquote{``with ChatGPT, you need to give much more context, I need to describe in detail about what x,y-axes should be, but here I can just provide with UI,''} and further commented that UI + NL \pquote{``helped me in framing and structuring the different transformations that we need to do to get to that end result.''}



\bpstart{Iteration styles} Participants developed their own iteration styles working with \df--\autoref{fig:user-workflows} illustrates their organization of data threads in their workspaces upon completing the study tasks. Although our participant pool of 8 did not encompass all possible users' data exploration styles with \df, we observed surprising behavior clusters and distinct approach differences. We characterize participants' iteration styles based on their preferences between ``wider'' versus ``deeper'' tree structures, ``backtrack and revise'' versus ``follow up'' for providing new instructions to the AI, as well as their preferences for including intermediate tables in their threads.

\medskip

\noindent\emph{(Wide versus deep tree organizations):} From the high-level organization of data threads, one group of participants (P1, P3, P5, P7, P8) preferred to branch out more often with shorter data threads than the other group (P2, P4, P6), who preferred to create fewer but longer data threads instead.
P1 explained that their preference of more branches with shorter data threads came from their coding style of \pquote{``creating as many as transformation as I can from one single table without generating derived tables''} to keep the system's memory usage minimal and keep the workspace \pquote{``terse.''}
On the other hand, P2, who preferred longer data threads, mentioned \pquote{``I definitely like to be able to just work on top of that and like going forward by just giving a new prompt, because it remembers the context prior to the last one. It ends up generating the right data and visualization.''} P2 further commented that \pquote{``going back created too much branching''} and they preferred to use longer threads to just provide updates for \pquote{``smooth train of thoughts.''} To effectively work with long threads, P4 organized their exploration process thoughtfully, as they were \pquote{``using the prompts as my anchor, so, when I wanted to figure out where I wanted to go, it was the prompts that I was looking for.''} 

\medskip

\noindent\emph{(Backtracking versus following-up):} We observed interesting patterns in participants' preferences when creating new charts or correcting unexpected results: some preferred revising previous instructions (evident from workflows with more self-loop arrows), while others favored following up (characterized by more forward arrows and intermediate gray data nodes). The first group, represented by P1, P2 and P3, preferred to go back and re-issue prompts, either to enrich the previous data to support multiple target visualizations (indicated by yellow nodes with multiple target charts), or to update the data to correct unexpected results. For example, when P1 and P3 worked on coloring the top 20 earning majors with their major categories (chart-C4 in \autoref{fig:study-tasks}), they revised the previous prompt (\pprompt{``show only top 20 majors based on median salary''} $\rightarrow$ \pprompt{``show only top 20 majors based on median salary, include major category''} by P1) to include \code{Major\_Category} so that both old and the new charts can be created from the same data. To correct a mistake they made in creating chart-M7 (they forgot to instruct the AI to show only top 20 movies), P3 chose to go back and revise their previous prompt (\pprompt{``calculate the profit ratio per movie (worldwide\_gross/budget) after 2000''} $\rightarrow$
\pprompt{``calculate the profit ratio per movie (worldwide\_gross/budget) after 2000 and display the top 20 higher profit ratio movies''}). P1 commented that \pquote{``I don’t like to pollute my workspace''} and \pquote{``I like to keep my workspace as clean as possible.''} P3 mentioned that their preference of revision came from the concept of building a \pquote{``global expanded dataset''} so that \pquote{``[when I] need to calculate the new thing or see a new visual I can come back to the new expanded data set.''}

On the other hand, another group, represented by P4, P5, P6, and P7, preferred not only to issue follow-up instructions for new charts but also to provide updates with very brief instructions at each step, creating many intermediate nodes along the way (gray nodes in \autoref{fig:user-workflows}). For example, P5 created chart-M7 (top movies with highest profit ratio colored by genre) in five steps: \textit{``filter movies after year 2000''} $\rightarrow$ \textit{``show top 5 highest profit ratio''} $\rightarrow$ \textit{``bring back movie''} (i.e., the {Movie} field)  $\rightarrow$ \textit{``show top 10''} $\rightarrow$ \emph{``calculate profit ratio,''} creating four intermediate nodes. %Whereas, P1 created chart-M7 with just one instruction from \textsf{movies} with \textit{``calculate the profit ratio as a ratio of worldwide gross to production budget for the top 20 movies.''} 
P5 noted that \pquote{``probably redoing would make sense, but if I can think that I can build on top of that, there is no value for me to go back and start from that, [which] kind of nullify these things [I have done],''} as they preferred to keep their work around. P6 mentioned that they adapt their iteration style based on the type of mistakes they encountered: \pquote{``if it is something intermediate where I've made the mistake, I'll go [create a new instruction] and fix the previous step''} but when it \pquote{``is a totally new kind of visualization I have in my mind''} or \pquote{``if it is something I missed altogether, I will just cancel the whole thing and start from scratch.''}

\medskip

\noindent\emph{(Choices of data to iterate on):} Participants had different strategies deciding which previous data/charts to use to create new charts. P1 chose to derive the new chart from a previous chart that shares similar visual design. For example, P1 created chart-M9 from M7 since they are both bar charts showing top ranked movies, despite one is based on profit while another is based on profit ratio. 
In a different fashion, P2, P4 and P5 often branch out based on similarity of computations used. For example, P2 created chart-M7 about movies with highest profit ratio based on chart-M6 showing profit ratio trends for each genre over time, as they shared the same computation ``profit ratio.'' 
P2 explained their data-centric approach was because they \pquote{``prefer to have more control over the data as opposed to the chart later on.''} They also appreciated that \df \pquote{``sort of brings together both data-centric and chart-centric people.''}

\bpstart{Prompt styles} Prompts created by participants are all short (less than 20 words). We observed that participants created diverse styles of prompts, both in terms of how they phrase the instruction (e.g., question, command) and the subject they asked (e.g., describing expected visual output or output data property, providing computation formula). The most common style of prompts is imperative commands, that either describe the transformation to be conducted or the property of the desired output. For example, to filter top earning movies, participants used prompts \pprompt{``show only top 20''} [P6] and\pprompt{``filter top 10 movies based on median profit''} [P5]. Participants also used command-style prompts for describing computations (e.g., \pprompt{``calculate ratio of worldwide\_gross by production\_budget''} [P5]) and for visual updates (e.g., \pprompt{``color by major category''} [P8]. 
We also observed that some participants prompted with questions (e.g., \pprompt{``can you show only the top 5 countries in terms of increases?''} [P7]), 
%, \pprompt{``What are top movies after 2000 with highest profit ratio?''} [P2]
or prompted in a chat style (e.g., \pprompt{``Good. We need to now find the Median profit ratio each year for each genre''} [P2].%, \pprompt{``I want to calculate the nuclear energy differences from 2000 and 2020 for each country''} [P3 during practice tasks]). 

One participant, P5, had a distinct prompting style, that directly asked the AI to add, mutate, or retrieve columns on top of the previous data. For example, P5 asked \pprompt{``bring back major category''} to create chart-C4 from C3, \pprompt{``divide by 100,000''} for updating profit units, \pprompt{``bring back release\_date''} before they used a follow-up command \pprompt{``show only year greater than 2,000''} to filter movies by date. P7 preferred to use more verbose prompts to reiterate the computation they intended to achieve whenever they mentioned the concept. For example, to ensure that the AI would not interpret the computation differently, they copy/pasted the formula to the prompt whenever they mentioned profit ratio --- \pprompt{``median profit ratio (worldwide\_gross/production\_budget) by year and by genre.''} P6, on the other hand, preferred to use no additional prompts and provided more descriptive field names. For example, to create chart-C5, they mapped ``\code{percentage\_of\_women\_of\_Total\_Men\_and Women}'' to $x$-axis, \code{Median\_Salary} to $y$, and provided no prompt in the input box. In fact, we observed that \df can reliably transform data with self-explanatory field names (e.g., ``\code{renewable energy percentage}'', ``\code{women percentage}'', and ``\code{difference between 2020 and 2000}'') without any additional prompts. Some participants' preference for using shorter names and additional (short) prompts was \pquote{``to minimize the error space [for AI]''} [P7]. %Participants (e.g., P4, P7) also mentioned that \textit{``when I was starting out, I wanted to make sure it is doing the right thing, but as I'm working with it for a long period of time, I think sort of the trust on the system increases''} [P4], thus their prompting style could adapt.

%\bpstart{Iteration styles}
%\df lets users develop their own iteration strategies. We observed three major distinct styles of iteration, in terms of which tables or charts participants chose to derive a new chart. 

%The first type of users preferred to achieve a particular chart through small, incremental changes from an existing chart that shared either similar data fields or similar chart configuration. For example, P2 and P3 chose to create the line chart showing profit ratio trends over time on top of the bar chart showing the average profit ratio per genre, and next visualized movies with highest profit ratio further on top, since they share the same derived field \code{profit ratio}. P2 mentioned \textit{``I definitely like to be able to just work on top of that and like going forward by just giving a new prompt, because it remembers the context prior to the last one, it ends up generating the right data and visualization.''} P2 further commented that they did not like too much branching: \textit{``...felt that it would be harder to go back to the source and fix every single time.''} P7 also preferred incremental changes, but with a focus on visual similarity as opposed to data similarity.
%Similarly, P3 didn't like to go back to earlier stages, preferring to use only the most recent iteration and continually issue new prompts to achieve the desired results.

%In contrast, the second type of users preferred to go back and re-issue a prompt to achieve all the changes from the initial data as succinctly as possible. %This was in direct contrast to the style of work that P2 and P3 preferred. 
%For example, P1 mentioned that \textit{``[I] like keeping it as terse as possible that will get me the right result.''} %P4 felt like if there was no computational linking between successive stages of iteration, then 'there's no value in going back to a previous iteration' and in fact, used iterations primarily to figure out a single prompt which would achieve the results from the original dataset.
%P4 also felt that sometimes it was more productive to just start over from the original dataset throwing out all iterations, especially when they failed to produce a desired outcome: \textit{``when we had all of those failures, I went back to the original base dataset and then frame my question there.''}
%\textit{``But when I went back to the original, so when we had all of those failures, I went back to the original base data set, and then I framed my question there, right.''}

%The third type of users primarily think about the iterations in terms for adding (or retrieving) columns from the dataset. P5 preferred to first instruct \df to add/remove columns from an existing data (e.g., bring back fields that might have been dropped in previous iterations as needed, or add a new field required for the desired chart), and then create visualization from the right data.

%Other users, (P7 for instance) used previous charts for iteration. 'I prefer to modify existing plot...' and focused on the plot rather than the data itself.

%\bpstart{Organization of iteration history}  When asked about their rationale behind branching strategies, all participants agreed that data threads are essential for managing iteration histories. Regarding their preferred organization style, P1 mentioned \textit{``I don’t like to pollute my workspace''}  and \textit{``I like to keep my workspace as clean as possible''} and thus they always chose to backtrack and fix previous instructions when encountering undesired results. P2, who mentioned \textit{``going back created too much branching''} instead preferred to follow through. P4 used prompts to help navigate iterations to find the one they were looking for: \textit{``I was using the prompts as my anchor to figure out where I wanted to go.''} P8 found it sometimes difficult to iterate in \df because data threads were \textit{``linear instead of hierarchical''}: they preferred a tree-view data thread organization, where they could scan quickly through the entire branching tree for a dataset, its transformations and visualizations and then collapse branches that were not of interest for the current goals. 

\bpstart{Verification} To proceed through iterative exploration, or repeat/correct a step, participants needed to understand the chart and verify that the transformation was performed correctly. Most of the time, participants spotted unintended output easily through incorrect patterns in rendered visualizations. This happened especially when there were differences in visual encoding (e.g., when P5 incorrectly mapped \code{release\_date} to the $x$-axis instead of \code{year} on chart-M5), cardinality (e.g., when P6 incorrectly asked the AI to color the bars by \code{major} instead of \code{major\_category} for chart-C6), or high-level patterns (e.g., when P7 requested \code{women} versus \code{median\_salary} for chart-C7, leading to results based on the count of women instead of the percentage). When the transformation is straightforward, participants visually inspected the chart and data to verify correctness. For example, after P3 asked \pprompt{``filter the year after 2000''} to show only profit ratio trends for movies after 2000 (chart-M8), they checked the $x$-axis domain and compared the generated chart with the pre-filtered one. Similarly, after P2 input \pprompt{``filter results to top 20 by major''} to find the highest earning majors (chart-C5), they referred to the previous chart with all of the majors' \code{median salary} sorted to check filtering correctness. %P2 mentioned \emph{``for simpler queries as opposed to a bit more complex queries, it was easier for me to check, in these scenarios that, hey, this is what it looks like --- it should be sorted, these are the top values that it's getting---like we went and checked the the median salary for each department thing [for chart-C3].''}

To check whether unobvious computations were done correctly (e.g., whether the LLM computed profit ratio correctly), different participants' background impacted how they validated the results: participants either referred to (1) explanations of the code, (2) the actual code (even if they are non-python programmers), or (3) values in the result table to check correctness. P3 mentioned \pquote{``as an expert, I like to see the prompt to the model, and then the code generated; but as a business user, I would imagine using more data, chart, and explanations.''} while P4 commented \pquote{``[explanation] steps were really, really helpful in terms of figuring out whether it is doing the right thing as to what I'm asking it to do. That and also the data chart underneath.''} P7 noted that, for trust, the definition of a new field is more crucial than the actual code: \pquote{``I just want to make sure that definition, like profit ratio, when I check in, I only look at those definitions if they are correct. I'm less worried about the real coding piece.''} Thus, they use code explanations frequently to check definitions. Meanwhile, P7 stated that they felt some pressure from the study environment not to spend too much time understanding code for which they were not familiar with, but they would trust code more. We also observed participants who developed trust in a workflow (by examining code and data tables) when it was straightforward, and then, they assumed the more complicated transformations built on top of these steps worked. 

%Several people commented on the desire to have the system generate code, but then to manually update that code to either achieve iterations or adjust the code to correct. P3: 'During my work, if ... I can edit the coding here because that will be like ... [the] model is giving me something.'

% P8: 'The ability to look at the Python code gave me the confidence to know it was doing the right thing, like I don't use Python, but I've been programming for many years so that the Python code looks like a lot of other code that I've seen.’

% P8 also looked at the shape/size of the resultant data: 'No, the the first thing I look at is just the general shape of the data. Like I look at the graph with the graph looks too wide, too choppy... If it's an up and down thing that you saw, that usually means... the time slice wrong in some way like my group by is off in some way, so those are the obvious ones and then the then the other ones are less obvious. So what I'm looking the first thing I do is I ballpark it and I say this is this is this even in the ballpark? '

% P7 stated that they preferred to use code rather than explanations of the code, but in the study, they used almost exclusively the explanations. They stated that they felt some pressure from the study environment not to spend too much time understanding code for which they were not familiar.

%Several people commented on the desire to have the system generate code, but then to manually update that code to either achieve iterations or adjust the code to correct. P3: 'During my work, if ... I can edit the coding here because that will be like ... [the] model is giving me something.'

\bpstart{Additional Feedback} Several users noted potential improvements of \df. P1 commented on how small interface variations might give different affordances. For instance, \pquote{``if there was a large view for data threads, it would encourage me to do more transformations and do more branching.''} P3 mentioned that they prefer the AI to ask the user to disambiguate when the intent is unclear rather than trying to solve the task with unclear specification. P7 used instructions that were very detailed and sometimes incorrect, which in turn, made iteration more difficult, since it was difficult to incrementally modify these instructions. We discussed the potential of having templates or AI feedback for instruction crafting to reduce errors.
