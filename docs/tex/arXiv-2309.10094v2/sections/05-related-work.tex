\section{Related Work}
\label{sec:related-work}

Data Formulator builds on top of prior research in visualization authoring tools, data transformation tools, and code generation techniques.

\bpstart{Visualization Grammars and Tools} The grammar of graphics~\cite{DBLP:books/daglib/0024564} first introduces the representation of visualizations based on chart types and encodings of data columns to their visual channels. Many high-level grammars are designed to realize this idea. For example, ggplot2~\cite{wickham2009ggplot2} is a charting library in R based on visual encodings. Vega-Lite~\cite{satyanarayan2017vegalite} and its Python-wrapper Altair~\cite{vanderplas2018altair} extend the traditional grammar of graphics design with rules for layered and multi-view displays, as well as interactions, and Animated Vega-Lite~\cite{zong2022animated} further extends it to support animations. These grammars hide low-level implementation details and are concise. Therefore, they are generally preferred for the rapid creation of visualization in exploratory settings over toolkits and libraries like Protovis~\cite{bostock2009protovis}, Atlas~\cite{liu2021atlas}, and D3~\cite{bostock2011d3} that are designed for more expressive and novel visualization authoring. High-level grammars inspire interactive visualization tools like Tableau~\cite{stolte2002query}, Power BI, Lyra~\cite{satyanarayan2014lyra}, Charticulator~\cite{ren2019charticulator}, and Data Illustrator~\cite{liu2018data}. These tools adopt a shelf-configuration design: authors map data columns to visual encoding ``shelves'' often using the drag-and-drop interaction, and enerate specifications in high-level grammars to render visualizations. 
%They also extend the grammar of graphics approach with the ability to edit glyph, constrain layouts, and specify interactions to improve the expressiveness of visualization designs. 
These grammars and tools require that the input data is in a tidy format, where all variables to be visualized are columns of input data. Because this means authors often need to transform the data first to create any visualizations, Satyanarayan et al. recognized the automatic inferring or suggestions of appropriate transformations when necessary, as an important research problem~\cite{satyanarayan2019critical}. 

To reduce authors' efforts, visualization by demonstration~\cite{zong2020lyra,saket2016visualization,shen2022galvis} and by example~\cite{wang2021falx} tools are introduced. Lyra 2~\cite{zong2020lyra} generates interaction rules after authors perform an interaction on the visualization. VbD~\cite{saket2016visualization} lets users demonstrate transformations between different types of visualizations to produce new specifications. Although these approaches reduce the chart specification efforts, they require tidy input data. Falx~\cite{wang2021falx}, on the other hand, addresses the data transformation challenge with a visualization-by-example design. Falx lets authors specify visualizations via low-level example mappings from data points to primitive chart elements. However, Falx does not support derivation types of transformation because of its underlying programming-by-example algorithm limitations; its requirement to focus on low-level elements also introduces a challenging paradigm shift for users who are more familiar with tools that focus on high-level specifications~\cite{satyanarayan2017vegalite,stolte2002query}.

\new{Natural language interfaces~\cite{chen2022type,luo2021natural,poesia2022synchromesh,DBLP:conf/uist/GaoDALK15,DBLP:conf/chi/KimHA20,DBLP:conf/apvis/0004HJY21} enhance users' ability to author and reason about visualizations.  NCNet~\cite{luo2021natural} uses a Seq-to-Seq model to translate chart description texts into Vega-Lite specs. VisQA~\cite{DBLP:conf/chi/KimHA20} is a pipeline that leverages semantic parsing techniques~\cite{DBLP:conf/acl/PasupatL15} to provide atomic data-related answers based on its visualizations. NL4DV~\cite{narechania2020nl4dv} and Advisor~\cite{DBLP:conf/apvis/0004HJY21} generate visualizations based on user questions. To manage ambiguity in natural language inputs~\cite{srinivasan2021collecting}, DataTone~\cite{DBLP:conf/uist/GaoDALK15} ranks solutions based on user preference history, and Pumice~\cite{DBLP:conf/uist/LiRJSMM19pumice} introduces a multi-modal approach that leverages examples to refine the initial ambiguous specification. Data Formulator's concept derivation interface is based on natural language. Data Formulator benefits from large language models' expressiveness~\cite{chen2021evaluating}, and manages ambiguity by restricting the target function type to columns-to-column mapping functions (as opposed to arbitrary data transformation scripts). In the future, more powerful language models can be plugged into Data Formulator to improve code generation quality.}

Data Formulator adopts the shelf-configuration approach like Tableau and Power BI, but it supports encoding from \emph{data concepts} to visual channels to address the data transformation burden. Because Data Formulator can automatically transform the input data based on the concepts used in the visualization specification, authors do not need to manually transform data. Furthermore, because Data Formulator's Chart Builder resembles tools like Power BI and Tableau, it lets the authors focus on high-level designs. Data Formulator's multi-modal interaction approach supports both derivation and reshaping tables. While Data Formulator currently focuses on standard visualization supported by Vega-Lite, its AI-powered concept-driven approach can also work with expressive and creative visualization design tools like StructGraph~\cite{tsandilas2020structgraphics} and Data Illustrator~\cite{liu2018data} to automate data transformations.

\bpstart{Data Transformation Tools} Libraries and tools like tidyverse~\cite{wickham2019tidyverse}, pandas~\cite{the_pandas_development_team_2023_7741580}, Potter's Wheel~\cite{raman2001potter}, Wrangler~\cite{kandel2011wrangler}, Tableau Prep, and Power Query are developed to support data transformation. They introduce operators to reshape, compute, and manipulate tabular data needed in data analysis. Automated data transformation tools, including programming-by-example tools\cite{polozov2015flashmeta,wang2017synthesizing,jin2017foofah} and initiative tools~\cite{jin2020auto,yan2020auto,beth2020mage,kandel2011wrangler}, are developed to reduce authors' specification effort. Data Formulator tailors key transformation operators from the tidyverse library (reshaping and derivation) for visualization authoring. Because the desired data shape changes with visualization goals, even with these tools, authors still need the knowledge and effort to first identify the desired data shape, and then switch tools to transform the data. Data Formulator bridges visual encoding and data transformation with data concepts to reduce this overhead.

\bpstart{Code Generation} Code generation models~\cite{chen2021evaluating,chowdhery2022palm,fried2022incoder} and program synthesis techniques~\cite{gulwani2017program,wang2017synthesizing,chaudhuri2021neurosymbolic,zhang2021interpretable} enable users to complete tasks without programming by using easier specifications, including natural language, examples, and demonstrations. Code generation models like Codex~\cite{chen2021evaluating}, PaLM~\cite{chowdhery2022palm}, and InCoder~\cite{fried2022incoder} are transformer-based causal language models (commonly referred to as LLMs) that complete texts from natural language prompts. These LLMs can generate expressive programs to solve competitive programming~\cite{li2022competition,hendrycks2021measuring}, data science~\cite{lai2022ds}, and software engineering tasks~\cite{barke2022grounded} from high-level descriptions. Programming-by-example~\cite{wang2021falx} and programming-by-demonstration~\cite{barman2016ringer,pu2022semanticon} tools can synthesize programs based on users' output examples or demonstrations that illustrate the computation process. Natural language approaches are highly expressive, but some tasks can be challenging to phrase. On the other hand, while programming-by-example techniques are precise, they are less expressive and do not scale to large programs as they require well-defined program spaces. Therefore, Data Formulator adopts a mixed-modality approach to solve the data transformation task. It leverages the Codex model~\cite{chen2021evaluating} for concept derivations and the example-based synthesis algorithm~\cite{wang2019visualization} for reshaping, which takes advantage of both approaches to reduce authors' specification overhead.

Because code generation techniques generalize programs from incomplete user specifications, generated programs are inherently ambiguous, and thus require disambiguation to identify a correct solution among candidates. Prior work proposes techniques to visualize the search process~\cite{zhang2021interpretable}, visualize code candidates~\cite{wang2021falx,xiong1912revealing}, and present distinguishing examples for authors to inspect~\cite{ji2020question}. Data Formulator provides feedback to the authors by presenting the generated code together with its execution results for them to inspect, select, and edit.